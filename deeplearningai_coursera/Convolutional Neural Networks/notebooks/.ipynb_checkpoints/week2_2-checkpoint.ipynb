{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jBHWs2vuBk9"
   },
   "source": [
    "### Networks in Networks and 1x1 Convolutions\n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/karenyyy/Coursera_and_Udemy/master/deeplearningai_coursera/Convolutional%20Neural%20Networks/images/27.png)\n",
    "\n",
    "\n",
    "__1x1 convolution operation__ is actually doing a pretty non-trival operation and it allows to shrink the number of channels in the volumes or keep it the same or even increase it if needed.\n",
    "\n",
    "\n",
    "\n",
    "### Inception Network Motivation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 738
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3018,
     "status": "ok",
     "timestamp": 1530806315721,
     "user": {
      "displayName": "Jiarong Ye",
      "photoUrl": "//lh4.googleusercontent.com/-q7b8noGUXEk/AAAAAAAAAAI/AAAAAAAAALM/dMDENHiMdoY/s50-c-k-no/photo.jpg",
      "userId": "110838105291424103468"
     },
     "user_tz": 240
    },
    "id": "r-zjHWeA1tHX",
    "outputId": "6543a1ca-c7f8-449b-e880-c0bb75afc7c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 224, 224, 64) 256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 224, 224, 64) 256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 224, 224, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 224, 224, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 224, 224, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 224, 224, 64) 256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 224, 224, 64) 36928       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 224, 224, 64) 102464      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 224, 224, 64) 256         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 224, 224, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 224, 224, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 224, 224, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 224, 224, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 224, 224, 256 0           batch_normalization_1[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 141,952\n",
      "Trainable params: 141,184\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,Dropout,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,concatenate\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D,AveragePooling2D\n",
    "\n",
    "def Conv2d_BN(x, nb_filter,kernel_size, padding='same',strides=(1,1),name=None):\n",
    "    if name is not None:\n",
    "        bn_name = name + '_bn'\n",
    "        conv_name = name + '_conv'\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "\n",
    "    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)\n",
    "    x = BatchNormalization(axis=3,name=bn_name)(x)\n",
    "    return x\n",
    "  \n",
    "def Inception(x,nb_filter):\n",
    "    branch1x1 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None)\n",
    "\n",
    "    branch3x3 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None)\n",
    "    branch3x3 = Conv2d_BN(branch3x3,nb_filter,(3,3), padding='same',strides=(1,1),name=None)\n",
    "\n",
    "    branch5x5 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None)\n",
    "    branch5x5 = Conv2d_BN(branch5x5,nb_filter,(5,5), padding='same',strides=(1,1),name=None)\n",
    "\n",
    "    branchpool = MaxPooling2D(pool_size=(3,3),strides=(1,1),padding='same')(x)\n",
    "    branchpool = Conv2d_BN(branchpool,nb_filter,(1,1),padding='same',strides=(1,1),name=None)\n",
    "\n",
    "    ## concatenate here to increase the feature dimension depth (axis = 3)\n",
    "    x = concatenate([branch1x1,branch3x3,branch5x5,branchpool],axis=3) \n",
    "\n",
    "    return x\n",
    "  \n",
    "inpt = Input(shape=(224,224,3))\n",
    "x = Inception(inpt,64)\n",
    "model = Model(inpt,x,name='inception')\n",
    "model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkvM4Tm0MJow"
   },
   "source": [
    "在网络结构的设计上，经常说DenseNet和Inception中更多采用的是concatenate操作，而ResNet更多采用的add操作，那么这两个操作有什么异同呢？\n",
    "\n",
    "concatenate操作是网络结构设计中很重要的一种操作，经常用于将特征联合，多个卷积特征提取框架提取的特征融合或者是将输出层的信息进行融合，而add层更像是信息之间的叠加。\n",
    "\n",
    "\n",
    "Resnet是做值的叠加，通道数是不变的，DenseNet是做通道的合并。你可以这么理解，add是描述图像的特征下的信息量增多了，但是描述图像的维度本身并没有增加，只是每一维下的信息量在增加，这显然是对最终的图像的分类是有益的。而concatenate是通道数的合并，也就是说描述图像本身的特征增加了，而每一特征下的信息是没有增加。\n",
    "\n",
    "在代码层面就是ResNet使用的都是add操作，而DenseNet使用的是concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyN6-FMK427Q"
   },
   "source": [
    "![](https://raw.githubusercontent.com/karenyyy/Coursera_and_Udemy/master/deeplearningai_coursera/Convolutional%20Neural%20Networks/images/28.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VDSR--Accurate Image Super-Resolution Using Very Deep Convolutional Networks\n",
    "\n",
    "论文链接：https://arxiv.org/abs/1511.04587\n",
    "\n",
    "论文code: https://github.com/huangzehao/caffe-vdsr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SRCNN虽然成功把CNN引入到SR中，但是依然受限于三个方面：\n",
    "- SRCNN依赖于一些比较小的感受野；\n",
    "- 训练收敛速度太慢；\n",
    "- 这个网络只能work for 一个单一尺度\n",
    "\n",
    "通过stack filters来获得一个比较大的感受野。最大达到41x41的感受野，在形式上，其实更有点像ResNet。通过一个global的residual connect来解决加深网络而导致的梯度问题。\n",
    "\n",
    "\n",
    "- Context: 本文通过stack small filters来进行获得一个比较大的感受野，最大达到41x41。事实上，大的感受野可以有效进行超分辨率重构。\n",
    "- Convergence: 本文通过residual learning来学习Input和Output之间的difference。由于仅学习残差，因此learning rate可以设置的比较大，可以更快的加速收敛。\n",
    "- Contribution: 作者通过stack filters来获得比较大的感受野，并通过high learning rate来加快收敛。然而deep 网络和high learning rate会出现梯度爆炸，因此作者提出residual-learning和gradient clipping\n",
    "\n",
    "ResNet和VDSR两者结构比较：\n",
    "\n",
    "![](../images/vdsr.jpeg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also a good example of how to construct a model in class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-ASguEWS3h_p"
   },
   "source": [
    "### Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def imread(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def imsave(image, filepath, result_dir):\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(), result_dir)):\n",
    "        os.makedirs(os.path.join(os.getcwd(), result_dir))\n",
    "    cv2.imwrite(os.path.join(os.getcwd(), result_dir), image*255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def checkimage(image):\n",
    "    cv2.imshow(\"test\", image)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modcrop(img, scale = 3):\n",
    "    if len(img.shape) == 3:\n",
    "        h, w, _ = img.shape\n",
    "        h = (h/scale)*scale\n",
    "        w = (w/scale)*scale\n",
    "        img = img[:h, :w]\n",
    "    else:\n",
    "        h, w,= img.shape\n",
    "        h = (h/scale)*scale\n",
    "        w = (w/scale)*scale\n",
    "        img = img[:h, :w]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def checkpoint_dir(config):\n",
    "    if config.is_train:\n",
    "        return os.path.join('./{}'.format(config.checkpoint_dir), \"train.h5\")\n",
    "    else:\n",
    "        return os.path.join('./{}'.format(config.checkpoint_dir), \"test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preprocess(path, scale = 3):\n",
    "    img = imread(path)\n",
    "    croped_img = modcrop(img, scale)\n",
    "    bicbuic_img = cv2.resize(label_,None,fx = 1.0/scale ,fy = 1.0/scale, interpolation = cv2.INTER_CUBIC)# Resize by scaling factor\n",
    "    input_ = cv2.resize(bicbuic_img,None,fx = scale ,fy=scale, interpolation = cv2.INTER_CUBIC)# Resize by scaling factor\n",
    "    return input_, label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def prepare_data(dataset=\"Train\",test_img=\"\"):\n",
    "    if test_img !=\"\":\n",
    "        data = [os.path.join(os.getcwd(),test_img)]\n",
    "    else:\n",
    "        data_dir = os.path.join(os.getcwd(), dataset) \n",
    "        data = glob.glob(os.path.join(data_dir, \"*.jpg\")) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_data(is_train, test_img):\n",
    "    if is_train:\n",
    "        data = prepare_data(dataset=\"Train\")\n",
    "    else:\n",
    "        data = prepare_data(dataset=\"Test\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## predefined parameters\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer(\"epoch\", 1500, \"Number of epoch\")\n",
    "flags.DEFINE_integer(\"image_size\", 41, \"The size of image input\")\n",
    "flags.DEFINE_integer(\"label_size\", 41, \"The size of image output\")\n",
    "flags.DEFINE_integer(\"c_dim\", 3, \"The size of channel\")\n",
    "flags.DEFINE_boolean(\"is_train\", True, \"if the train\")\n",
    "flags.DEFINE_integer(\"scale\", 3, \"the size of scale factor for preprocessing input image\")\n",
    "flags.DEFINE_integer(\"stride\", 41, \"the size of stride\") ##because output is 33 * 33\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoint\", \"Name of checkpoint directory\")\n",
    "flags.DEFINE_float(\"learning_rate\", 1e-4 , \"The learning rate\")\n",
    "flags.DEFINE_integer(\"batch_size\", 64, \"the size of batch\")\n",
    "flags.DEFINE_string(\"result_dir\", \"result\", \"Name of result directory\")\n",
    "flags.DEFINE_string(\"test_img\", \"\", \"test_img\")\n",
    "flags.DEFINE_float(\"clip_grad\", 1e-1 , \"The clip gradient number\")\n",
    "flags.DEFINE_integer(\"layer\", 20, \"the size of layer\")\n",
    "config = FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def make_sub_data(data, config):\n",
    "    sub_input_sequence = []\n",
    "    sub_label_sequence = []\n",
    "    ## preprocess one at a time \n",
    "    for i in range(len(data)):\n",
    "        if config.is_train:\n",
    "            input_, label_, = preprocess(data[i], config.scale) \n",
    "        else: \n",
    "            input_, label_, = preprocess(data[i], config.scale) \n",
    "        \n",
    "        if len(input_.shape) == 3: \n",
    "            h, w, c = input_.shape\n",
    "        else:\n",
    "            h, w = input_.shape\n",
    "        checkimage(input_) ## display to  see whether the input is correct\n",
    "        nx, ny = 0, 0\n",
    "        for x in range(0, h - config.image_size + 1, config.stride):\n",
    "            nx += 1; ny = 0\n",
    "            for y in range(0, w - config.image_size + 1, config.stride):\n",
    "                ny += 1\n",
    "\n",
    "                sub_input = input_[x: x + config.image_size, y: y + config.image_size] # 41 * 41\n",
    "                sub_label = label_[x: x + config.label_size, y: y + config.label_size] # 41 * 41\n",
    "\n",
    "                # Reshape the subinput and sublabel\n",
    "                sub_input = sub_input.reshape([config.image_size, config.image_size, config.c_dim])\n",
    "                sub_label = sub_label.reshape([config.label_size, config.label_size, config.c_dim])\n",
    "\n",
    "                # Normialize\n",
    "                sub_input =  sub_input / 255.0\n",
    "                sub_label =  sub_label / 255.0\n",
    "                \n",
    "                cv2.imshow(\"im1\",sub_input)\n",
    "                cv2.imshow(\"im2\",sub_label)\n",
    "                cv2.imshow(\"residual\",sub_input - sub_label)\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "                # Add to sequence\n",
    "                sub_input_sequence.append(sub_input)\n",
    "                sub_label_sequence.append(sub_label)\n",
    "        \n",
    "    # NOTE: The nx, ny can be ignore in train\n",
    "    return sub_input_sequence, sub_label_sequence, nx, ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def make_data_hf(input_, label_, config):\n",
    "    \"\"\"Make input data as h5 file format\"\"\"\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(),config.checkpoint_dir)):\n",
    "        os.makedirs(os.path.join(os.getcwd(),config.checkpoint_dir))\n",
    "\n",
    "    if config.is_train:\n",
    "        savepath = os.path.join(os.getcwd(), config.checkpoint_dir + '/train.h5')\n",
    "    else:\n",
    "        savepath = os.path.join(os.getcwd(), config.checkpoint_dir + '/test.h5')\n",
    "\n",
    "    with h5py.File(savepath, 'w') as hf:\n",
    "        #checkimage(input_[1])\n",
    "        hf.create_dataset('input', data=input_)\n",
    "        hf.create_dataset('label', data=label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\"Read h5 format data file\"\"\"\n",
    "    with h5py.File(path, 'r') as hf:\n",
    "        input_ = np.array(hf.get('input'))\n",
    "        label_ = np.array(hf.get('label'))\n",
    "        return input_, label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def merge(images, size, c_dim):\n",
    "    \"\"\"images is the sub image set, merge it\"\"\"\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h*size[0], w*size[1], c_dim))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j * h : j * h + h,i * w : i * w + w, :] = image\n",
    "        cv2.imshow(\"srimg\",img)\n",
    "        cv2.waitKey(0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def input_setup(config):\n",
    "    \"\"\"Read image files and make their sub-images and saved them as a h5 file format\"\"\"\n",
    "    # Load data path, if is_train False, get test data\n",
    "    data = load_data(config.is_train, config.test_img)\n",
    "    # Make sub_input and sub_label, if is_train false more return nx, ny\n",
    "    sub_input_sequence, sub_label_sequence, nx, ny = make_sub_data(data, config)\n",
    "    # Make list to numpy array. With this transform\n",
    "    arrinput = np.asarray(sub_input_sequence) # [?, 41, 41, 3]\n",
    "    arrlabel = np.asarray(sub_label_sequence) # [?, 41, 41, 3]\n",
    "    make_data_hf(arrinput, arrlabel, config)\n",
    "    return nx, ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     2,
     47,
     68,
     92,
     106
    ]
   },
   "outputs": [],
   "source": [
    "class VDSR(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 sess,\n",
    "                 image_size,\n",
    "                 label_size,\n",
    "                 layer,\n",
    "                 c_dim):\n",
    "        self.sess = sess\n",
    "        self.image_size = image_size\n",
    "        self.label_size = label_size\n",
    "        self.layer = layer\n",
    "        self.c_dim = c_dim\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.images = tf.placeholder(tf.float32, [None, self.image_size, self.image_size, self.c_dim], name='images')\n",
    "        self.labels = tf.placeholder(tf.float32, [None, self.label_size, self.label_size, self.c_dim], name='labels')\n",
    "        \n",
    "        \n",
    "        self.weights = {\n",
    "            'w_start': tf.Variable(tf.random_normal([3, 3, self.c_dim, 64], stddev =np.sqrt(2.0/9)), name='w_start'),\n",
    "            'w_end': tf.Variable(tf.random_normal([3, 3, 64, self.c_dim], stddev=np.sqrt(2.0/9/64)), name='w_end')\n",
    "        }\n",
    "\n",
    "        self.biases = {\n",
    "            'b_start': tf.Variable(tf.zeros([64], name='b_start')),\n",
    "            'b_end': tf.Variable(tf.zeros([self.c_dim], name='b_end'))\n",
    "        }\n",
    "\n",
    "        # Create very deep layer weight and bias\n",
    "        for i in range(2, self.layer): \n",
    "            self.weights.update({'w_%d' % i: tf.Variable(tf.random_normal([3, 3, 64, 64], stddev= np.sqrt(2.0/9/64)), name='w_%d' % i) })\n",
    "            self.biases.update({'b_%d' % i: tf.Variable(tf.zeros([64], name='b_%d' % i)) })\n",
    "            \n",
    "        self.pred = self.model()\n",
    "        self.loss = tf.reduce_mean(tf.square(self.labels - self.images - self.pred))\n",
    "        self.saver = tf.train.Saver() \n",
    "        \n",
    "    def model(self):\n",
    "        conv = []\n",
    "        conv.append(tf.nn.relu(tf.nn.conv2d(self.images, self.weights['w_start'], strides=[1,1,1,1], padding='SAME') + self.biases['b_start']))\n",
    "        for i in range(2, self.layer):\n",
    "            conv.append(tf.nn.relu(tf.nn.conv2d(conv[i-2], self.weights['w_%d' % i], strides=[1,1,1,1], padding='SAME') + self.biases['b_%d' % i]))\n",
    "        conv_end = tf.nn.conv2d(conv[i-1], self.weights['w_end'], strides=[1,1,1,1], padding='SAME') + self.biases['b_end'] # This layer don't need ReLU\n",
    "        return conv_end\n",
    "\n",
    "    def train(self, config):\n",
    "        nx, ny = input_setup(config)\n",
    "        data_dir = checkpoint_dir(config) ## the h5 file dir \n",
    "        input_, label_ = read_data(data_dir)\n",
    "        # Stochastic gradient descent with the standard backpropagation\n",
    "        # NOTE: learning rate decay\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(config.learning_rate, global_step * config.batch_size, len(input_)*100, 0.1, staircase=True)\n",
    "        # NOTE: Clip gradient\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        grad_and_value = opt.compute_gradients(self.loss)\n",
    "        clip = tf.Variable(config.clip_grad, name='clip') \n",
    "        capped_gvs = [(tf.clip_by_value(grad, -(clip), clip), var) for grad, var in grad_and_value]\n",
    "        self.train_op = opt.apply_gradients(capped_gvs, global_step=global_step)\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        counter = 0\n",
    "        time_ = time.time()\n",
    "        self.load(config.checkpoint_dir)\n",
    "        \n",
    "        # Train\n",
    "        if config.is_train:\n",
    "            print(\"Start Training...\")\n",
    "            for ep in range(config.epoch):\n",
    "                # Run by batch images\n",
    "                batch_idxs = len(input_) // config.batch_size\n",
    "                for idx in range(0, batch_idxs):\n",
    "                    batch_images = input_[idx * config.batch_size : (idx + 1) * config.batch_size]\n",
    "                    batch_labels = label_[idx * config.batch_size : (idx + 1) * config.batch_size]\n",
    "                    counter += 1\n",
    "                    _, err = self.sess.run([self.train_op, self.loss], feed_dict={self.images: batch_images, self.labels: batch_labels})\n",
    "\n",
    "                    if counter % 10 == 0:\n",
    "                        print(\"Epoch: [%2d], step: [%2d], time: [%4.4f], loss: [%.8f]\" % ((ep+1), counter, time.time()-time_, err ))\n",
    "                    if counter % 500 == 0:\n",
    "                        self.save(config.checkpoint_dir, counter)\n",
    "        # Test\n",
    "        else:\n",
    "            print(\"Start Testing...\")\n",
    "            result = self.pred.eval({self.images: input_}) + input_\n",
    "            image = merge(result, [nx, ny], self.c_dim)\n",
    "            checkimage(merge(result, [nx, ny], self.c_dim))\n",
    "            #checkimage(image_LR)\n",
    "            imsave(image, config.result_dir+'/result.png', config)\n",
    "            \n",
    "    def load_checkpoint(self, checkpoint_dir):\n",
    "        print(\"\\nReading Checkpoints.....\\n\\n\")\n",
    "        model_dir = \"%s_%s_%slayer\" % (\"vdsr\", self.label_size, self.layer)# give the model name by label_size\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        \n",
    "        # Check the checkpoint is exist \n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_path = str(ckpt.model_checkpoint_path) # convert the unicode to string\n",
    "            self.saver.restore(self.sess, os.path.join(os.getcwd(), ckpt_path))\n",
    "            print(\"\\n Checkpoint Loading Success! %s\\n\\n\"% ckpt_path)\n",
    "        else:\n",
    "            print(\"\\n! Checkpoint Loading Failed \\n\\n\")\n",
    "            \n",
    "    def save_checkpoint(self, checkpoint_dir, step):\n",
    "        model_name = \"VDSR.model\"\n",
    "        model_dir = \"%s_%s_%slayer\" % (\"vdsr\", self.label_size,self.layer)\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "             os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,\n",
    "                        os.path.join(checkpoint_dir, model_name),\n",
    "                        global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'nx' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-551515c2e1db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-551515c2e1db>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m      7\u001b[0m                       c_dim = FLAGS.c_dim)\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mvdsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-272e811851a4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## the h5 file dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-0780b806080b>\u001b[0m in \u001b[0;36minput_setup\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Make sub_input and sub_label, if is_train false more return nx, ny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msub_input_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_label_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_sub_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Make list to numpy array. With this transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0marrinput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_input_sequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [?, 41, 41, 3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8d13f19aa9f6>\u001b[0m in \u001b[0;36mmake_sub_data\u001b[0;34m(data, config)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# NOTE: The nx, ny can be ignore in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msub_input_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_label_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'nx' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def main(_): \n",
    "    with tf.Session() as sess:\n",
    "        vdsr = VDSR(sess,\n",
    "                      image_size = FLAGS.image_size,\n",
    "                      label_size = FLAGS.label_size,\n",
    "                      layer = FLAGS.layer,\n",
    "                      c_dim = FLAGS.c_dim)\n",
    "\n",
    "        vdsr.train(FLAGS)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "week2_2.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
