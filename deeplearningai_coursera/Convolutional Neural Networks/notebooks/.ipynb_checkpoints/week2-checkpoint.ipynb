{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05bfEF_iByF-"
   },
   "source": [
    "## Classic Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SA1VcmEK2jq"
   },
   "source": [
    "### Lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7fH_Q4q_CHII"
   },
   "source": [
    "\n",
    "![](https://raw.githubusercontent.com/karenyyy/Coursera_and_Udemy/master/deeplearningai_coursera/Convolutional%20Neural%20Networks/images/22.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "H7MMB4bYI5mU"
   },
   "outputs": [],
   "source": [
    "## Lenet\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "## mnist dataset\n",
    "train_x = train_set[0].reshape((-1,28,28,1))\n",
    "train_y = to_categorical(train_set[1])\n",
    " \n",
    "valid_x = valid_set[0].reshape((-1,28,28,1))\n",
    "valid_y = to_categorical(valid_set[1])\n",
    " \n",
    "test_x = test_set[0].reshape((-1,28,28,1))\n",
    "test_y = to_categorical(test_set[1])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32,(5,5),strides=(1,1),input_shape=(28,28,1),padding='valid',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64,(5,5),strides=(1,1),padding='valid',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(model.evaluate(test_x,test_y,batch_size=20,verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4a1027542c7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_path' is not defined"
     ]
    }
   ],
   "source": [
    "## load images\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "\n",
    "# train_path = ...\n",
    "# test_path = ...\n",
    "\n",
    "width = 32\n",
    "height = 32\n",
    "channel = 1\n",
    "\n",
    "\n",
    "def load_image(path):\n",
    "    path_list = [path+subdir for subdir in os.listdir(path) if os.path.isdir(path+subdir)]\n",
    "    images=[]\n",
    "    labels=[]\n",
    "    for index, path in enumerate(path_list):\n",
    "        for image in glob.glob(path+'/*.png'):\n",
    "            image = io.imread(image)\n",
    "            image = transform.resize(images, output_shape=(width, height, channel))\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "    return np.asarray(images, dtype = np.float32), np.asarray(labels, dtype=np.int32)\n",
    "\n",
    "train_data,train_label = load_image(train_path)\n",
    "test_data,test_label = load_image(test_path)\n",
    "\n",
    "train_data_index = np.arange(len(train_data))\n",
    "np.random.shuffle(train_data_index)\n",
    "test_data_index = np.arange(len(test_data))\n",
    "np.random.shuffle(test_data_index)\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,w,h,c],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None],name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Lenet Tensorflow\n",
    "\n",
    "def model(input_):\n",
    "    with tf.varibale_scope('layer1-conv'):\n",
    "        conv1_w = tf.get_variable('conv1_w', [5,5,c,6], initializer=tf.contrib.layers.xavier_initializer(seed=None))\n",
    "        conv1_b = tf.get_variable('conv1_b', [6], initializer=tf.contrib.layers.xavier_initializer(seed=None))\n",
    "        conv1 = tf.nn.conv2d(input_, conv1_w, strides=[1,1,1,1], padding='VALID')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_b))\n",
    "        \n",
    "    with tf.variable_scope('layer2-pool1'):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    with tf.variable_scope('layer3-conv2'):\n",
    "        conv2_weights = tf.get_variable('weight',[5,5,6,16],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable('bias',[16],initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1,conv2_weights,strides=[1,1,1,1],padding='VALID')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2,conv2_biases))\n",
    "    \n",
    "    with tf.variable_scope('layer4-pool2'):\n",
    "        pool2 = tf.nn.max_pool(relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    \n",
    "    ## convert pool2 to len(training_samples) x (the rest dimensions multiply together)\n",
    "    pool_shape = pool2.get_shape().as_list()\n",
    "    nodes = pool_shape[1]*pool_shape[2]*pool_shape[3]\n",
    "    reshaped = tf.reshape(pool2,[-1,nodes])\n",
    "    \n",
    "    ## then the full-connected layer\n",
    "    with tf.variable_scope('layer5-fc1'):\n",
    "        fc1_weights = tf.get_variable('weight',[nodes,120],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(0.001)(fc1_weights))\n",
    "        fc1_biases = tf.get_variable('bias',[120],initializer=tf.constant_initializer(0.1))\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped,fc1_weights) + fc1_biases)\n",
    "        fc1 = tf.nn.dropout(fc1,0.5)\n",
    "            \n",
    "    with tf.variable_scope('layer7-fc3'):\n",
    "        fc3_weights = tf.get_variable('weight',[84,10],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(0.001)(fc3_weights))\n",
    "        fc3_biases = tf.get_variable('bias',[10],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        logit = tf.matmul(fc2,fc3_weights) + fc3_biases\n",
    "    \n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model accuracy\n",
    "def accuracy(x, y):\n",
    "    logit = model(x)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "    cost = cross_entropy + tf.add_n(tf.get_collection('losses'))\n",
    "    train_op = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "    correct_pred = tf.equal(tf.cast(tf.arg_max(y, 1), dtype=tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get batch\n",
    "def get_batch(data,label,batch_size):\n",
    "    for start_index in range(0,len(data)-batch_size+1,batch_size):\n",
    "        slice_index = slice(start_index,start_index+batch_size)\n",
    "        yield data[slice_index],label[slice_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## train mdoel\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    train_num = 10\n",
    "    batch_size = 64\n",
    "\n",
    "    for i in range(train_num):\n",
    "        \n",
    "        train_loss=0\n",
    "        train_acc=0\n",
    "        batch_num = 0\n",
    "        \n",
    "        for mini_batch_x, mini_batch_y in get_batch(train_data,train_label,batch_size):\n",
    "            _,err,acc = sess.run([train_op,cost,accuracy],feed_dict={X:mini_batch_x,\n",
    "                                                                     Y:mini_batch_y})\n",
    "            train_loss+=err\n",
    "            train_acc+=acc\n",
    "            batch_num+=1\n",
    "        print(\"train loss:\",train_loss/batch_num)\n",
    "        print(\"train acc:\",train_acc/batch_num)\n",
    "\n",
    "        test_loss,test_acc,batch_num = 0, 0, 0\n",
    "        for test_data_batch,test_label_batch in get_batch(test_data,test_label,batch_size):\n",
    "            err,acc = sess.run([loss,accuracy],feed_dict={x:test_data_batch,y_:test_label_batch})\n",
    "            test_loss+=err;test_acc+=acc;batch_num+=1\n",
    "        print(\"test loss:\",test_loss/batch_num)\n",
    "        print(\"test acc:\",test_acc/batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:71: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:74: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50] Loss: 0.45478, Acc: 0.00, Time: 70.4 s\n",
      "[2/50] Loss: 0.10020, Acc: 0.00, Time: 82.6 s\n",
      "[3/50] Loss: 0.07597, Acc: 0.00, Time: 73.0 s\n",
      "[4/50] Loss: 0.06365, Acc: 0.00, Time: 72.8 s\n",
      "[5/50] Loss: 0.05612, Acc: 0.00, Time: 75.0 s\n"
     ]
    }
   ],
   "source": [
    "## Lenet Pytorch\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epoches = 50\n",
    "\n",
    "trans_img = transforms.ToTensor()\n",
    "\n",
    "trainset = MNIST('./data', train=True, transform=trans_img, download=True)\n",
    "testset = MNIST('./data', train=False, transform=trans_img, download=True)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# build network\n",
    "class Lenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5, stride=1, padding=0),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(400, 120),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "lenet = Lenet()\n",
    "\n",
    "criterian = nn.CrossEntropyLoss(size_average=False)\n",
    "optimizer = optim.SGD(lenet.parameters(), lr=learning_rate)\n",
    "\n",
    "# train\n",
    "for i in range(epoches):\n",
    "    since = time.time()\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    for (img, label) in trainloader:\n",
    "        img = Variable(img)\n",
    "        label = Variable(label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = lenet(img)\n",
    "        loss = criterian(output, label)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        _, predict = torch.max(output, 1)\n",
    "        correct_num = (predict == label).sum()\n",
    "        running_acc += correct_num.data[0]\n",
    "\n",
    "    running_loss /= len(trainset)\n",
    "    running_acc /= len(trainset)\n",
    "    print(\"[%d/%d] Loss: %.5f, Acc: %.2f, Time: %.1f s\" %(i+1, epoches, running_loss, 100*running_acc, time.time()-since))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6WFfOxSKIpcI"
   },
   "source": [
    "\n",
    "![](https://raw.githubusercontent.com/karenyyy/Coursera_and_Udemy/master/deeplearningai_coursera/Convolutional%20Neural%20Networks/images/21.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "V6lZVE6GUqC-"
   },
   "outputs": [],
   "source": [
    "## AlexNet\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "\n",
    "model = Sequential()\n",
    "model.add()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGjgeTrXIti4"
   },
   "source": [
    "\n",
    "![](https://raw.githubusercontent.com/karenyyy/Coursera_and_Udemy/master/deeplearningai_coursera/Convolutional%20Neural%20Networks/images/22.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## VGG 19\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import inspect\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "\n",
    "class Vgg19:\n",
    "    def __init__(self, vgg19_npy_path=None):\n",
    "        if vgg19_npy_path is None:\n",
    "            path = inspect.getfile(Vgg19)\n",
    "            path = os.path.abspath(os.path.join(path, os.pardir))\n",
    "            path = os.path.join(path, \"vgg19.npy\")\n",
    "            vgg19_npy_path = path\n",
    "            print(vgg19_npy_path)\n",
    "\n",
    "        self.data_dict = np.load(vgg19_npy_path, encoding='latin1').item()\n",
    "\n",
    "    def build(self, rgb):\n",
    "        rgb_scaled = rgb * 255.0\n",
    "\n",
    "        # Convert RGB to BGR\n",
    "        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=rgb_scaled)\n",
    "        assert red.get_shape().as_list()[1:] == [224, 224, 1]\n",
    "        assert green.get_shape().as_list()[1:] == [224, 224, 1]\n",
    "        assert blue.get_shape().as_list()[1:] == [224, 224, 1]\n",
    "        bgr = tf.concat(axis=3, values=[\n",
    "            blue - VGG_MEAN[0],\n",
    "            green - VGG_MEAN[1],\n",
    "            red - VGG_MEAN[2],\n",
    "        ])\n",
    "        assert bgr.get_shape().as_list()[1:] == [224, 224, 3]\n",
    "\n",
    "        self.conv1_1 = self.conv_layer(bgr, \"conv1_1\")\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, \"conv1_2\")\n",
    "        self.pool1 = self.max_pool(self.conv1_2, 'pool1')\n",
    "\n",
    "        self.conv2_1 = self.conv_layer(self.pool1, \"conv2_1\")\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, \"conv2_2\")\n",
    "        self.pool2 = self.max_pool(self.conv2_2, 'pool2')\n",
    "\n",
    "        self.conv3_1 = self.conv_layer(self.pool2, \"conv3_1\")\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, \"conv3_2\")\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, \"conv3_3\")\n",
    "        self.conv3_4 = self.conv_layer(self.conv3_3, \"conv3_4\")\n",
    "        self.pool3 = self.max_pool(self.conv3_4, 'pool3')\n",
    "\n",
    "        self.conv4_1 = self.conv_layer(self.pool3, \"conv4_1\")\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, \"conv4_2\")\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, \"conv4_3\")\n",
    "        self.conv4_4 = self.conv_layer(self.conv4_3, \"conv4_4\")\n",
    "        self.pool4 = self.max_pool(self.conv4_4, 'pool4')\n",
    "\n",
    "        self.conv5_1 = self.conv_layer(self.pool4, \"conv5_1\")\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, \"conv5_2\")\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, \"conv5_3\")\n",
    "        self.conv5_4 = self.conv_layer(self.conv5_3, \"conv5_4\")\n",
    "        self.pool5 = self.max_pool(self.conv5_4, 'pool5')\n",
    "\n",
    "        self.fc6 = self.fc_layer(self.pool5, \"fc6\")\n",
    "        assert self.fc6.get_shape().as_list()[1:] == [4096]\n",
    "        self.relu6 = tf.nn.relu(self.fc6)\n",
    "\n",
    "        self.fc7 = self.fc_layer(self.relu6, \"fc7\")\n",
    "        self.relu7 = tf.nn.relu(self.fc7)\n",
    "\n",
    "        self.fc8 = self.fc_layer(self.relu7, \"fc8\")\n",
    "\n",
    "        self.prob = tf.nn.softmax(self.fc8, name=\"prob\")\n",
    "\n",
    "        self.data_dict = None\n",
    "\n",
    "    def avg_pool(self, bottom, name):\n",
    "        return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def max_pool(self, bottom, name):\n",
    "        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def conv_layer(self, bottom, name):\n",
    "        with tf.variable_scope(name):\n",
    "            filt = self.get_conv_filter(name)\n",
    "\n",
    "            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "            conv_biases = self.get_bias(name)\n",
    "            bias = tf.nn.bias_add(conv, conv_biases)\n",
    "\n",
    "            relu = tf.nn.relu(bias)\n",
    "            return relu\n",
    "\n",
    "    def fc_layer(self, bottom, name):\n",
    "        with tf.variable_scope(name):\n",
    "            shape = bottom.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(bottom, [-1, dim])\n",
    "\n",
    "            weights = self.get_fc_weight(name)\n",
    "            biases = self.get_bias(name)\n",
    "\n",
    "            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)\n",
    "\n",
    "            return fc\n",
    "\n",
    "    def get_conv_filter(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name=\"filter\")\n",
    "\n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name=\"biases\")\n",
    "\n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name=\"weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mK7hkp0NIwPO"
   },
   "source": [
    "\n",
    "![](https://raw.githubusercontent.com/karenyyy/Coursera_and_Udemy/master/deeplearningai_coursera/Convolutional%20Neural%20Networks/images/23.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6abQQOGIymu"
   },
   "source": [
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/karenyyy/Coursera_and_Udemy/master/deeplearningai_coursera/Convolutional%20Neural%20Networks/images/24.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wG7-QJiUI03S"
   },
   "source": [
    "\n",
    "> Why do residual networks work?\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/karenyyy/Coursera_and_Udemy/master/deeplearningai_coursera/Convolutional%20Neural%20Networks/images/25.png)\n",
    "\n",
    "\n",
    "\n",
    "> What would be the problems in very deep plain net __without the residual of the skip connection__?\n",
    "\n",
    "\n",
    "__It is very difficult for the network to choose parameters that learn even the identity function which is why a lot of layers end up making the result worse rather than making the result better__\n",
    "\n",
    "__And residual network works since it is very easy for the extra layers to learn the identity function and then a lot of the time may even improve the performance__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/karenyyy/Coursera_and_Udemy/master/deeplearningai_coursera/Convolutional%20Neural%20Networks/images/26.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wToY8D4zIoei"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "week2.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
