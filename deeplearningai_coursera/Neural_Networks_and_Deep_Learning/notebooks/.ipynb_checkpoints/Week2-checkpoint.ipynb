{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Logistic Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "- Logistic network is practically a binary classification\n",
    "\n",
    "__Note: reshape each image as one single vector__\n",
    "![](../images/8.png)\n",
    "\n",
    "- Notation:\n",
    "\n",
    "![](../images/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\text{ Given the input feature x, where } x \\in \\mathbb{R}^{n_x}$$\n",
    "\n",
    "$$\\text{  Goal: } \\hat y = P(y=1 \\mid x), \\text{ where }  \\hat y \\in [0,1]$$\n",
    "\n",
    "$$\\text{ parameter1: } w \\in \\mathbb{R}^{n_x \\cdot ?}  $$\n",
    "\n",
    "$$\\text{ parameter2: } b \\in \\mathbb{R}^{1 \\cdot m}$$\n",
    "\n",
    "$$\\text{ Output: } \\hat y = \\sigma( w^{T}x + b) = \\sigma (z), \\text { where } z= w^{T}x+b, \\text { and } \\sigma(z) = \n",
    "\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "$$\\text{ If z is large, } \\sigma (z) \\approx \\frac{1}{1+0} = 1 $$\n",
    "\n",
    "$$\\text{ If z is small (neg), } \\sigma (z) \\approx \\frac{1}{1+\\infty} \\approx 0 $$\n",
    "\n",
    "#### alternative notation:\n",
    "$$x_0 = 1, x \\in \\mathbb{R}^{n_x+1}, \\hat y= \\sigma (\\theta ^{T} x)$$\n",
    "\n",
    "$$\\text{ where } \\theta = \\begin{pmatrix} \\theta_0\n",
    "\\\\ \\theta_1\n",
    "\\\\ \\theta_2\n",
    "\\\\ \\theta_3\n",
    "\\\\ ...\n",
    "\\\\ \\theta_{n_x}\n",
    "\\end{pmatrix}, \\theta_0 \\rightarrow b, (\\theta_1, \\theta_2,..., \\theta_{n_x}) \\rightarrow w $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Reg Cost  Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat y^{(i)} = \\sigma (w^{T}x+b), \\text{ where } \\sigma(z^{(i)}) = \\frac{1}{1+e^{-z^{(i)}}} $$\n",
    "\n",
    "Note: __DO NOT DO THE FOLLOWING:__\n",
    "\n",
    "$$\\text{ Loss(error) function: } L(\\hat y, y) = \\frac{1}{2} (\\hat y - y)^2$$\n",
    " \n",
    "___Because when it comes to learn the parameters, we can find that the optimization problem would become non-convex, so we could end up with optimization problem with multiple local optima, so gradient descent may not find the global optimum___\n",
    "\n",
    "In Logistic Reg, the standard __loss__ function is:\n",
    "\n",
    "- Negative log-likelihood (cross-entropy error)\n",
    "\n",
    "$$L(\\hat y, y) = - \\left (y \\log \\hat y + (1-y) \\log (1- \\hat y) \\right )$$\n",
    "\n",
    "$$\\text { If y=1, } L(\\hat y, y) = - \\log \\hat y, \\text {so our goal: } \\min (L) \\rightarrow \\max(\\hat y) $$\n",
    "$$\\text { If y=0, } L(\\hat y, y) = - \\log (1- \\hat y), \\text {so our goal: }  \\min (L) \\rightarrow \\min(\\hat y) $$\n",
    "\n",
    "__Total cost function__:\n",
    "$$J(w,b) = \\frac{1}{m} \\sum^m_{i=1} L(\\hat y^{(i)}, y^{(i)}) = - \\frac{1}{m} \\sum^m_{i=1} \\left (y^{(i)} \\log \\hat y^{(i)} + (1-y^{(i)}) \\log (1- \\hat y^{(i)}) \\right )$$\n",
    "\n",
    "__Note1:__\n",
    "- The loss function computes the error for a single training example; \n",
    "- the cost function is the average of the loss functions of the entire training set.\n",
    "\n",
    "__Note2: Other loss functions__\n",
    "\n",
    "- Hinge (SVM, soft margin) \n",
    "$$L(y) = \\max(0, 1- y), \\text{ where if } y>1, \\text { then error = 0, the classification output is correct. }  $$\n",
    "- Squared loss (linear regression) \n",
    "- Exponential loss (Boosting) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Cost Function:\n",
    "$$J(w,b) = \\frac{1}{m} \\sum^m_{i=1} L(\\hat y^{(i)}, y^{(i)}) = - \\frac{1}{m} \\sum^m_{i=1} \\left (y^{(i)} \\log \\hat y^{(i)} + (1-y^{(i)}) \\log (1- \\hat y^{(i)}) \\right )$$\n",
    "\n",
    "__Goal: find w, b that minimize J(w,b)__ \n",
    "\n",
    "![](../images/10.png)\n",
    "\n",
    "__Initialization: for logistic regression__\n",
    "- almost any initialization method works (__usually init as 0__)\n",
    "- random initialization also works\n",
    "\n",
    "__How Gradient Descent works?__\n",
    "- start at the initial point and then takes a step in the steepest downhill direction\n",
    "\n",
    "![](../images/11.png)\n",
    "\n",
    "$$\\text{Repeated: } w=w-\\alpha \\frac{\\partial J(w, b)}{\\partial w},b=b-\\alpha \\frac{\\partial J(w, b)}{\\partial b}$$\n",
    "\n",
    "Eg: \n",
    "![](../images/12.png)\n",
    "\n",
    "- at this point, the slope here of the derivative would be negative \n",
    "- so the gradient descent update would subtract alpha times a negative number, ending up slowly __increasing w__\n",
    "- so w gets bigger and bigger with successive iteration and gradient descent\n",
    "- __Vice Versa when you initialize w on the right side__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Computing derivatives\n",
    "![](../images/13.png)\n",
    "\n",
    "![](../images/15.png)\n",
    "\n",
    "- GD on m examples\n",
    "    - pseudo code:\n",
    "    \n",
    "![](../images/14.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
