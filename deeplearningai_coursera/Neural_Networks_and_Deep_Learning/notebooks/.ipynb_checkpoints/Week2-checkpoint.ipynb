{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Logistic Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "- Logistic network is practically a binary classification\n",
    "\n",
    "__Note: reshape each image as one single vector__\n",
    "![](../images/8.png)\n",
    "img<src=\"../images/8.png>\n",
    "- Notation:\n",
    "\n",
    "![](../images/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\text{ Given the input feature x, where } x \\in \\mathbb{R}^{n_x}$$\n",
    "\n",
    "$$\\text{  Goal: } \\hat y = P(y=1 \\mid x), \\text{ where }  \\hat y \\in [0,1]$$\n",
    "\n",
    "$$\\text{ parameter1: } w \\in \\mathbb{R}^{n_x \\cdot ?}  $$\n",
    "\n",
    "$$\\text{ parameter2: } b \\in \\mathbb{R}^{1 \\cdot m}$$\n",
    "\n",
    "$$\\text{ Output: } \\hat y = \\sigma( w^{T}x + b) = \\sigma (z), \\text { where } z= w^{T}x+b, \\text { and } \\sigma(z) = \n",
    "\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "$$\\text{ If z is large, } \\sigma (z) \\approx \\frac{1}{1+0} = 1 $$\n",
    "\n",
    "$$\\text{ If z is small (neg), } \\sigma (z) \\approx \\frac{1}{1+\\infty} \\approx 0 $$\n",
    "\n",
    "#### alternative notation:\n",
    "$$x_0 = 1, x \\in \\mathbb{R}^{n_x+1}, \\hat y= \\sigma (\\theta ^{T} x)$$\n",
    "\n",
    "$$\\text{ where } \\theta = \\begin{pmatrix} \\theta_0\n",
    "\\\\ \\theta_1\n",
    "\\\\ \\theta_2\n",
    "\\\\ \\theta_3\n",
    "\\\\ ...\n",
    "\\\\ \\theta_{n_x}\n",
    "\\end{pmatrix}, \\theta_0 \\rightarrow b, (\\theta_1, \\theta_2,..., \\theta_{n_x}) \\rightarrow w $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Reg Cost  Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat y^{(i)} = \\sigma (w^{T}x+b), \\text{ where } \\sigma(z^{(i)}) = \\frac{1}{1+e^{-z^{(i)}}} $$\n",
    "\n",
    "Note: __DO NOT DO THE FOLLOWING:__\n",
    "\n",
    "$$\\text{ Loss(error) function: } L(\\hat y, y) = \\frac{1}{2} (\\hat y - y)^2$$\n",
    " \n",
    "___Because when it comes to learn the parameters, we can find that the optimization problem would become non-convex, so we could end up with optimization problem with multiple local optima, so gradient descent may not find the global optimum___\n",
    "\n",
    "In Logistic Reg, the standard __loss__ function is:\n",
    "\n",
    "- Negative log-likelihood (cross-entropy error)\n",
    "\n",
    "$$L(\\hat y, y) = - \\left (y \\log \\hat y + (1-y) \\log (1- \\hat y) \\right )$$\n",
    "\n",
    "$$\\text { If y=1, } L(\\hat y, y) = - \\log \\hat y, \\text {so our goal: } \\min (L) \\rightarrow \\max(\\hat y) $$\n",
    "$$\\text { If y=0, } L(\\hat y, y) = - \\log (1- \\hat y), \\text {so our goal: }  \\min (L) \\rightarrow \\min(\\hat y) $$\n",
    "\n",
    "__Total cost function__:\n",
    "$$J(w,b) = \\frac{1}{m} \\sum^m_{i=1} L(\\hat y^{(i)}, y^{(i)}) = - \\frac{1}{m} \\sum^m_{i=1} \\left (y^{(i)} \\log \\hat y^{(i)} + (1-y^{(i)}) \\log (1- \\hat y^{(i)}) \\right )$$\n",
    "\n",
    "__Note1:__\n",
    "- The loss function computes the error for a single training example; \n",
    "- the cost function is the average of the loss functions of the entire training set.\n",
    "\n",
    "__Note2: Other loss functions__\n",
    "\n",
    "- Hinge (SVM, soft margin) \n",
    "$$L(y) = \\max(0, 1- y), \\text{ where if } y>1, \\text { then error = 0, the classification output is correct. }  $$\n",
    "- Squared loss (linear regression) \n",
    "- Exponential loss (Boosting) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Cost Function:\n",
    "$$J(w,b) = \\frac{1}{m} \\sum^m_{i=1} L(\\hat y^{(i)}, y^{(i)}) = - \\frac{1}{m} \\sum^m_{i=1} \\left (y^{(i)} \\log \\hat y^{(i)} + (1-y^{(i)}) \\log (1- \\hat y^{(i)}) \\right )$$\n",
    "\n",
    "__Goal: find w, b that minimize J(w,b)__ \n",
    "\n",
    "![](../images/10.png)\n",
    "\n",
    "__Initialization: for logistic regression__\n",
    "- almost any initialization method works (__usually init as 0__)\n",
    "- random initialization also works\n",
    "\n",
    "__How Gradient Descent works?__\n",
    "- start at the initial point and then takes a step in the steepest downhill direction\n",
    "\n",
    "![](../images/11.png)\n",
    "\n",
    "$$\\text{Repeated: } w=w-\\alpha \\frac{\\partial J(w, b)}{\\partial w},b=b-\\alpha \\frac{\\partial J(w, b)}{\\partial b}$$\n",
    "\n",
    "Eg: \n",
    "![](../images/12.png)\n",
    "\n",
    "- at this point, the slope here of the derivative would be negative \n",
    "- so the gradient descent update would subtract alpha times a negative number, ending up slowly __increasing w__\n",
    "- so w gets bigger and bigger with successive iteration and gradient descent\n",
    "- __Vice Versa when you initialize w on the right side__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Logistic Regression Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Computing derivatives\n",
    "![](../images/13.png)\n",
    "\n",
    "![](../images/15.png)\n",
    "\n",
    "- GD on m examples\n",
    "    - pseudo code:\n",
    "    \n",
    "![](../images/14.png)   \n",
    "\n",
    "But, here, if there are too many features (when n not equal to 2), then to avoid the many for loops, should use vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Vectorization is the art of getting rid of explicit folders in your code.\n",
    "\n",
    "![](../images/16.png)\n",
    "\n",
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001032114028930664"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "number = 1000000\n",
    "a=np.random.rand(number)\n",
    "b=np.random.rand(number)\n",
    "\n",
    "# vectorized:\n",
    "tic=time.time()\n",
    "c=np.dot(a,b) # a^T * b\n",
    "toc=time.time()\n",
    "toc-tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37551069259643555"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for loop:\n",
    "tic=time.time()\n",
    "c=0\n",
    "for i in range(number):\n",
    "    c+=a[i]*b[i] \n",
    "toc=time.time()\n",
    "toc-tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Note1: if 'number' is significantly large, the difference of vectorization and for loop would also be enormous.__\n",
    "\n",
    "__Note1: Both GPU and CPU have parallelization instructions, called SIMD instructions (single instruction, multiple data)__\n",
    "\n",
    "if you use a built-in function such as the __np.dot__ function, or any else that does not require explicitly implementing a for loop, it enables numpy to take much better advantage to do computations faster, and it is true for both computations CPUs and computations on GPUs(since GPU really good at SIMD)\n",
    "\n",
    "### __Take-away: Avoid explicit for-loops__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../images/17.png)\n",
    "\n",
    "> A subtlety in python: here b is a real number (1 x 1 matrix), python automatically takes this real number B and expands it out to in this case, a 1 x M row vector, it is called __Broadcasting__ in python.\n",
    "\n",
    "- Vectorizing Logistic Regression\n",
    "![](../images/18.png)\n",
    "![](../images/19.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 18, 18, 18])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# broadcasting example:\n",
    "A=np.array([[1,2,3,4],\n",
    "          [5,6,7,8],\n",
    "          [4,3,2,1],\n",
    "          [8,7,6,5]])\n",
    "A.sum(axis=0) # top to bottom sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 26, 10, 26])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1) # left to right sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../images/20.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18, 18, 18, 18]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal = A.sum(axis=0) \n",
    "cal_reshaped = cal.reshape(1,4)\n",
    "cal_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.55555556, 11.11111111, 16.66666667, 22.22222222],\n",
       "       [27.77777778, 33.33333333, 38.88888889, 44.44444444],\n",
       "       [22.22222222, 16.66666667, 11.11111111,  5.55555556],\n",
       "       [44.44444444, 38.88888889, 33.33333333, 27.77777778]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A/cal_reshaped * 100 # calculate the precentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.55555556, 11.11111111, 16.66666667, 22.22222222],\n",
       "       [27.77777778, 33.33333333, 38.88888889, 44.44444444],\n",
       "       [22.22222222, 16.66666667, 11.11111111,  5.55555556],\n",
       "       [44.44444444, 38.88888889, 33.33333333, 27.77777778]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A/cal * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE & Neg Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/21.png)\n",
    "\n",
    "> assumption: \n",
    "\n",
    "$$\\text{ all training examples } \\overset{iid}{\\sim} $$\n",
    "\n",
    "$$P(\\text{ labels in training set }) = \\prod_{i=1}^m P(y^{(i)} \\mid x^{(i)})$$\n",
    "\n",
    "$$\\log P(\\text{ labels in training set }) = \\log \\prod_{i=1}^m P(y^{(i)} \\mid x^{(i)})$$\n",
    "\n",
    "$$\\log P(\\text{ labels in training set }) = \\sum^{m}_{i=1} \\log P(y^{(i)} \\mid x^{(i)}) = \\sum^{m}_{i=1} [-L(\\hat y^{(i)}, y^{(i)})]$$\n",
    "\n",
    "$$\\text{ Neg Log Likelihood : } L(\\hat y^{(i)}, y^{(i)})$$\n",
    "\n",
    "> MLE here: \n",
    "\n",
    "$$\\max (\\log P(\\text{ labels in training set })) = \\max \\sum^{m}_{i=1} \\log P(y^{(i)} \\mid x^{(i)}) = \\max \\sum^{m}_{i=1} [-L(\\hat y^{(i)}, y^{(i)})]$$\n",
    "\n",
    "\n",
    "$$=> \\min \\sum^{m}_{i=1} L(\\hat y^{(i)}, y^{(i)})$$\n",
    "\n",
    "> So:\n",
    "$$ \\min (\\text{ cost function }) \\rightarrow \\min(\\text{ Neg Log Likelihood } ) \\rightarrow \\max(- \\text{ Neg Log Likelihood } ) \\rightarrow MLE$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
