{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Name Entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### One-Hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../images/1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Why not use a standard network?\n",
    "\n",
    "![](../images/2.png)\n",
    "\n",
    "__Problems:__\n",
    "\n",
    "- the input and output might thbe different lengths and different examples;\n",
    "- __a naive neural network architecture like this, it doesn't share features learned across different positions of texts__\n",
    "- the input layer would be really large\n",
    "- A weight matrix of this first layer will end up having an enormous number of parameters\n",
    "\n",
    "![](../images/3.png)\n",
    "\n",
    "- RNN scans through the data from left to right\n",
    "- The parameters it uses for each time step are shared\n",
    "\n",
    "__Process:__\n",
    "\n",
    "- when making the prediction for y3, it gets info not only from x3 but also the info from x1 and x2 \n",
    "    - because the info on x1 can pass through activation functions that connected each 2 layers to help to predict with y3\n",
    "\n",
    "__Weakness:__\n",
    "\n",
    "- Only use the info that is earlier appeared in the sequence to make predictions\n",
    "    - when predicting y3, it does not use x4, x5, x6 ... (appear later)\n",
    "        - it is a problem because in a sentence:   words follows later are more useful fo name detecting this case\n",
    "        \n",
    "![](../images/4.png) \n",
    "\n",
    "\n",
    "> How to fix this problem?\n",
    "\n",
    "Introduce __Bidirectional RNN (BRNN)__\n",
    "\n",
    "__Forward Propagation__\n",
    "\n",
    "![](../images/5.png)\n",
    "\n",
    "![](../images/6.png)\n",
    "\n",
    "\n",
    "__Cost Function__\n",
    "\n",
    "![](../images/7.png)\n",
    "\n",
    "\n",
    "After computing the total cost, then back prop:\n",
    "\n",
    "![](../images/8.png)\n",
    "\n",
    "### Different types fo RNN\n",
    "\n",
    "\n",
    "![](../images/9.png)\n",
    "\n",
    "![](../images/10.png)\n",
    "\n",
    "![](../images/11.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Language model tells what the probability of a sentence is(how likely does it make snese?)__\n",
    "\n",
    "![](../images/13.png)\n",
    "\n",
    "Training set: large corpus of english text\n",
    "\n",
    "- First Step: __Tokenize the sentence__\n",
    "    - that means forming a vocabulary\n",
    "    - then map each of these words to one-hot vectors\n",
    "    - model when sentences end: add a extra token called a EOS (End of Sentence)\n",
    "    - replace unknown words in the sentence with 'UNK' (unknown)\n",
    "- Next Step: build a RNN to model the chance of these different sentences\n",
    "\n",
    "![](../images/14.png)\n",
    "\n",
    "![](../images/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Sampling a sequence from a trained RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Recap that a sequence model models the chances of any particular sequence of words as follows, so what we like to do is sample from this distribution to generate noble sequences fo words.\n",
    "\n",
    "- First Step: sample what is the first word that you want your model to generate\n",
    "    - your first time stamp will have some max probability over possible outputs\n",
    "- Second Step: randomly sample according to the softmax distribution\n",
    "\n",
    "![](../images/16.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Vanishing Gradients RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Fail to capture very long-term dependencies\n",
    "\n",
    "- The cat, which ..., was ...\n",
    "- The cats, which ..., were ...\n",
    "\n",
    "> Why?\n",
    "\n",
    "> Vanishing  gradient while training\n",
    "\n",
    "__Because of this problem, the basic RNN model has many local influences, meaning that the output is mainly influenced by values close to it.__\n",
    "\n",
    "> How to detect gradient explosion?\n",
    "\n",
    "- __'Nans'__ in your results, indicating that results of a numerical overflow in your NN computation\n",
    "    - Fix: __apply gradient clipping__. Look at your gradient vectors, and if it is bigger than some threshold, _rescale some of the gradients so that it is not too big_\n",
    "\n",
    "> How to fix vanishing gradient?\n",
    "\n",
    "Use __GRU__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../images/17.png)\n",
    "\n",
    "Using the cat example above:\n",
    "\n",
    "- C: memory cell, to memorize whether it is single or plural so that when it gets much further into the sentence it can still work under consideration\n",
    "\n",
    "$$c^{<t>} = a^{<t>}$$\n",
    "\n",
    "- At each time step, overwrite c with :\n",
    "\n",
    "$$\\hat c^{<t>} = \\tanh (w_c [c^{t-1}, x^{t}] + b_c)$$\n",
    "\n",
    "$$\\Gamma_u = \\sigma (w_u[c^{t-1}, x^{t}] + b_u)$$\n",
    "\n",
    "$$c^{<t>} = 1 \\text{ if single } $$\n",
    "\n",
    "$$c^{<t>} = 0 \\text{ if plural } $$\n",
    "\n",
    "- And then the GRU unit will memorize the value of the c all the way until verb appears\n",
    "- The job of the gate (Gamma u) is to decide when to update these values\n",
    "    - In particular, when we see the phrase, 'the cat', then a new concept the especially subject of the sentence cat. So this would be a good time to update this bit\n",
    "    - When we done using it, say, after 'The cat... was...' this sentence is finished, then it is time to trigger the Gamma to forget the subject 'cat'\n",
    "    \n",
    "$$\\Gamma_{u} = 1 \\text{ if subject remains } $$\n",
    "\n",
    "$$\\Gamma_{u} = 0 \\text{ if sentence ends and subject changes } $$    \n",
    "\n",
    "$$c^{<t>} = \\Gamma_u * \\hat c^{<t>} + (1-\\Gamma_u) * c^{<t-1>}$$\n",
    "\n",
    "![](../images/18.png)\n",
    "\n",
    "![](../images/19.png)\n",
    "\n",
    "\n",
    "### Full GRU\n",
    "\n",
    "![](../images/20.png)\n",
    "\n",
    "$$ \\Gamma r \\text{ tells how relevant is } c^{<t-1>}$$\n",
    "\n",
    "![](../images/21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../images/22.png)\n",
    "\n",
    "![](../images/23.png)\n",
    "\n",
    "![](../images/24.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/25.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
