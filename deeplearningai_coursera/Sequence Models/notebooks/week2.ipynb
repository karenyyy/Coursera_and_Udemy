{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Note: the problem of one-hot is that it can not address to the order of the words in a sentence.__\n",
    "\n",
    "### Featurized representation: word embedding\n",
    "\n",
    "#### words get embeddedt as a point into high-dimensional space\n",
    "\n",
    "![](../images/26.png)\n",
    "\n",
    "> How can we determine two words are similar to each other by reading tons of corpus?\n",
    "\n",
    "> Take the word embedding and apply it to the task, though which we can have a much smaller training set\n",
    "\n",
    "__For a name entity task, use BRNN__\n",
    "\n",
    "#### Transfer Learning and Word Embeddings\n",
    "\n",
    "- learn word embeddings from large text corpus (1-100B words)\n",
    "    - or download pre-trained embedding online\n",
    "- transform embedding to new task with smaller training set\n",
    "- continue to fine tune the word embeddings with new data\n",
    "\n",
    "![](../images/27.png)\n",
    "\n",
    "### Cosine similarity - most commonly used \n",
    "\n",
    "$$sim(u,v) = \\frac{u^Tv}{||u||_2||v||_2}$$\n",
    "\n",
    "### Embedding Matrix\n",
    "\n",
    "When implementing an algorithm to learn a word embedding, what you end up learning is an embedding matrix.\n",
    "\n",
    "![](../images/28.png)\n",
    "\n",
    "![](../images/29.png)\n",
    "\n",
    "if the window length is 4, so the words used to predict the next word is the previous 4.\n",
    "\n",
    "![](../images/30.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-grams\n",
    "\n",
    "![](../images/31.png)\n",
    "\n",
    "$$p(t | c) = \\frac{e^{\\theta_t^Te_c}}{\\sum_{j=1}^{10000} e^{\\theta_j^Te_c}}$$\n",
    "\n",
    "Problem:\n",
    "- computational speed\n",
    "    - have to carry out a sum over all 10000 words in the vocab\n",
    "    \n",
    "### negative sampling\n",
    "\n",
    "![](../images/32.png)\n",
    "\n",
    "> How to choose the negative examples?\n",
    "\n",
    "- A heuristic approach:\n",
    "\n",
    "$$P(w_i) = \\frac{f(w_i)^{\\frac{3}{4}}}{\\sum_{j=1}^{10000} f(w_j)^{\\frac{3}{4}}}$$\n",
    "\n",
    "\n",
    "### GloVe Algorithm\n",
    "\n",
    "#### global vectors for word repersentation\n",
    "\n",
    "$$X_{ij} = \\text{ times i } \\rightarrow t \\text{ appears in context of } j \\rightarrow c$$\n",
    "\n",
    "$$X_{ij} = X_{ji} \\rightarrow \\text{ captures the counts of i and j appear together}$$\n",
    "\n",
    "what the GloVe model does is that it optimize the following:\n",
    "\n",
    "$$\\min (\\theta_i^T e_j - \\log X_{ij})^2 \\rightarrow \\min (\\theta_t^T e_c - \\log X_{tc})^2, \\text{ where t: target word; c: context word }$$\n",
    "\n",
    "$$\\text{How related are words t and c are measured by how often they occur with each other}$$\n",
    "\n",
    "> How to solve theta and e?\n",
    "\n",
    "$$\\text{solve for the parameters } \\theta \\text{ and } e \\text{ using gradient descent to minimize the sum}$$\n",
    "\n",
    "$$\\text{weighting term: } f(X_{ij}) $$\n",
    "\n",
    "$$\\min \\sum_{i=1}^{10000} \\sum_{j=1}^{10000} f(X_{ij})(\\theta_i^T e_j +b_i +b_j'  \\log X_{ij})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/33.png)\n",
    "\n",
    "![](../images/34.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
