{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Intro to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../images/1.png)\n",
    "\n",
    "![](../images/2.png)\n",
    "\n",
    "![](../images/3.png)\n",
    "\n",
    "![](../images/4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Applications of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../images/5.png)\n",
    "\n",
    "![](../images/6.png)\n",
    "\n",
    "![](../images/7.png)\n",
    "\n",
    "![](../images/8.png)\n",
    "\n",
    "![](../images/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('procyonid.n.01'), Synset('carnivore.n.01'), Synset('placental.n.01'), Synset('mammal.n.01'), Synset('vertebrate.n.01'), Synset('chordate.n.01'), Synset('animal.n.01'), Synset('organism.n.01'), Synset('living_thing.n.01'), Synset('whole.n.02'), Synset('object.n.01'), Synset('physical_entity.n.01'), Synset('entity.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "panda = wn.synset(\"panda.n.01\")\n",
    "print([_ for _ in panda.closure(lambda x: x.hypernyms())]) # closure used as apply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/10.png)\n",
    "\n",
    "- Since we can not calculate the word similarities using one-hot vectors\n",
    "\n",
    "Therefore we use another approach, by calculating distributional similarity based on representation. __We can get a lot of value by representing a word by means of its neighbors__\n",
    "\n",
    "\n",
    "### Basic idea of learning neural neetwork word embeddings\n",
    "\n",
    "$$p(context\\:\\mid w_t) = ...$$\n",
    "\n",
    "which has a loss function, e.g.,\n",
    "\n",
    "$$J = 1- p(w_{i \\text{ where } i \\neq t } \\mid w_t )$$\n",
    "\n",
    "where we will:\n",
    "\n",
    "- look at many positions t in a big language corpus\n",
    "- keep adjusting the vector representations of words to minimize the loss\n",
    "\n",
    "### Main idea of Word2vec\n",
    "\n",
    "- Two algorithms\n",
    "    - __Skip-grams__ (SG): Predict context words given target (position independent)\n",
    "    - __Continuous Bag of Words (CBOW)__: Predict target word from bag-of-words context\n",
    "\n",
    "- Two (moderately efficient) training methods\n",
    "    - hierarchial softmax\n",
    "    - negative sampling\n",
    "    \n",
    "![](../images/11.png)\n",
    "\n",
    "\n",
    "### Details of word2vec\n",
    "\n",
    "For each word t=1 ... T, predict surrounding words in a window of radius m of every word.\n",
    "\n",
    "Objective function: Maximize the probability of any context word given the current center word.\n",
    "\n",
    "$$J'(\\theta) = \\prod^T_{t=1} \\prod _{-m \\le j \\le m, j \\neq 0} p(w_{t+j} \\mid w_t; \\theta)$$\n",
    "\n",
    "- Negative Log Likelihood\n",
    "\n",
    "$$J(\\theta) = - \\frac{1}{T} \\sum^T_{t=1} \\sum_{-m \\le j \\le m, j \\neq 0} \\log p(w_{t+j} \\mid w_t)$$\n",
    "\n",
    "$$\\text{ where } \\theta \\text{ represents all variables we will optimize } $$\n",
    "\n",
    "$$p(o \\mid c) = \\frac{exp (u_o^T v_c)}{\\sum^v_{w=1} exp(u_w^T v_c)}, \\text{ where o: outside, c: center}$$\n",
    "\n",
    "![](../images/12.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
